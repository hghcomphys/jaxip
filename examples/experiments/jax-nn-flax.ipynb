{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796881e5-5af2-4239-a2fa-466ea4a41f46",
   "metadata": {},
   "source": [
    " Useful links:\n",
    " - [Flax_basics](https://flax.readthedocs.io/en/latest/guides/flax_basics.html)\n",
    " - [Training-Loop-in-JAX](https://wandb.ai/jax-series/simple-training-loop/reports/Writing-a-Training-Loop-in-JAX-FLAX--VmlldzoyMzA4ODEy)\n",
    " - [Kaggle example](https://www.kaggle.com/code/nilaychauhan/digit-recognizer-using-jax-flax/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701b692-e6bd-4863-b71d-3a9bc9232114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env JAX_ENABLE_X64=1\n",
    "%env JAX_PLATFORM_NAME=cpu\n",
    "# %env JAX_DISABLE_JIT=1\n",
    "# %env JAX_DEBUG_NANS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8b42e-467d-4bae-b725-17c1c2efa098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, lax, jit\n",
    "from flax import linen as nn\n",
    "from typing import Sequence, Callable\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "import jax.numpy as jnp\n",
    "from collections import defaultdict\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafebfb-0bc9-4d66-9f29-d36561c4ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "\n",
    "def generate_x(shape):\n",
    "    return 5 * jnp.pi * jnp.asarray(np.random.uniform(size=shape))\n",
    "\n",
    "def target(x):\n",
    "    return jnp.sin(x)*x\n",
    "\n",
    "x = generate_x((8, 1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785602d-2050-4e11-a305-793359ae67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [nn.Dense(feat) for feat in self.features]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d56bd-73dd-45d4-b08a-409298705eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(features=[16, 32, 64, 1])\n",
    "params = model.init(key2, x)['params']\n",
    "print(type)\n",
    "jax.tree_map(lambda x: x.shape, params) # Check the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cf372-7f04-446c-b435-b02fcbfdac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.apply({'params': params}, x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c2713c-3924-4d01-93b5-e3c8b27d00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_state(\n",
    "    model, \n",
    "    random_key, \n",
    "    shape, \n",
    "    learning_rate,\n",
    ") -> train_state.TrainState:\n",
    "    \n",
    "    # Initialize the Model\n",
    "    variables = model.init(random_key, jnp.ones(shape))\n",
    "    \n",
    "    # Create the optimizer\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    # optimizer = optax.sgd(0.0001, 0.9)\n",
    "    \n",
    "    # Create a State\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn = model.apply,\n",
    "        tx=optimizer,\n",
    "        params=variables['params'],\n",
    "    )\n",
    "\n",
    "state = init_train_state(\n",
    "    model, key2, (16, 1), 0.001\n",
    ")\n",
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc9b27-52bb-4613-85a6-f26fdaca3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(*, logits, labels):\n",
    "    return ((labels - logits)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affbd9e-9e2d-4612-a18e-599dee14b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(*, logits, labels):\n",
    "    loss = mse_loss(logits=logits, labels=labels)\n",
    "    accuracy = jnp.sqrt(loss)\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc825e5-57cd-4d45-87e2-be1d6bb05a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step(\n",
    "    state: train_state.TrainState, \n",
    "    batch: jnp.ndarray,\n",
    "):\n",
    "    x, y = batch\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, x)\n",
    "        loss = mse_loss(logits=logits, labels=y)\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "    grads, logits = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, labels=y)\n",
    "    \n",
    "    return state, metrics\n",
    "\n",
    "@jit\n",
    "def eval_step(\n",
    "    state: train_state.TrainState, \n",
    "    batch: jnp.ndarray,\n",
    "):\n",
    "    x, y = batch\n",
    "    logits = state.apply_fn({'params': state.params}, x)\n",
    "    metrics = compute_metrics(logits=logits, labels=y)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12164a63-100d-4247-9908-d88e7685a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "for epoch in range(500):\n",
    "    for _ in range(64): # loop over batches\n",
    "        x_ = generate_x((32, 1))\n",
    "        batch = x_, target(x_) \n",
    "        state, metrics = train_step(state, batch)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={metrics['loss']}\")\n",
    "        # print(state.params['layers_0']['bias'])\n",
    "    history['epoch'].append(epoch)\n",
    "    history['metrics_train'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9baec-ed61-4809-9dfb-345f9180363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['epoch'], [v['loss'] for v in history['metrics_train']])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19229d-5931-4e4b-b450-bb9f7690ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_x((512, 1))\n",
    "y_pred = model.apply({'params': state.params}, x)\n",
    "y_true = target(x)\n",
    "print(jnp.mean((y_pred - y_true)**2))\n",
    "\n",
    "plt.scatter(x, y_pred, label=\"neural network\")\n",
    "plt.scatter(x, y_true, label=\"target fucntion\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8cf43-901d-42f7-88ad-0ea5b4452a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_true, y_pred)\n",
    "plt.plot(y_true, y_true, c=\"r\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e6c9a-8f8d-4924-ba73-cde600475849",
   "metadata": {},
   "source": [
    "### Train multiple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950bcf4-d0fe-47fa-9132-40571d4550fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frozendict import frozendict\n",
    "from typing import Tuple\n",
    "\n",
    "@jit\n",
    "def train_step_multiple_model(\n",
    "    state: Tuple[train_state.TrainState], \n",
    "    batch: jnp.ndarray,\n",
    "):\n",
    "    x, y = batch\n",
    "\n",
    "    def loss_fn(params: Tuple[frozendict]):\n",
    "        logits = jnp.array(0.0)\n",
    "        for s, p in zip(state, params):\n",
    "            logits += s.apply_fn({'params': p}, x)\n",
    "        loss = mse_loss(logits=logits, labels=y)\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "    grads, logits = grad_fn(tuple(s.params for s in state))\n",
    "    state = tuple(s.apply_gradients(grads=g) for s, g in zip(state, grads))\n",
    "    metrics = compute_metrics(logits=logits, labels=y)\n",
    "    \n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1929992-affa-48fd-9a47-a938107ceafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "model = tuple(Net(features=[8, 16, 32, 1]) for _ in range(N))\n",
    "random_keys = jax.random.split(jax.random.PRNGKey(0), N)\n",
    "state = tuple(init_train_state(m, key, (16, 1), 0.001) for m, key in zip(model, random_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc1250-674c-4ef7-89a4-8f1fa0372565",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "for epoch in range(500):\n",
    "    for _ in range(64): # loop over batches\n",
    "        x_ = generate_x((32, 1))\n",
    "        batch = x_, target(x_) \n",
    "        state, metrics = train_step_multiple_model(state, batch)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={metrics['loss']}\")\n",
    "        # print(state.params['layers_0']['bias'])\n",
    "    history['epoch'].append(epoch)\n",
    "    history['metrics_train'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747597c2-a37f-4ac4-bfa0-39185d3b68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['epoch'], [v['loss'] for v in history['metrics_train']])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb0b6f-1d63-42b0-8539-fd44227a6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_x((512, 1))\n",
    "y_pred = sum(s.apply_fn({'params': s.params}, x) for s in state[:])\n",
    "y_true = target(x)\n",
    "print(jnp.mean((y_pred - y_true)**2))\n",
    "\n",
    "plt.title(\"Multiple model training\")\n",
    "plt.scatter(x, y_pred, label=\"neural network\")\n",
    "plt.scatter(x, y_true, label=\"target fucntion\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87040504-e4b9-40d6-a735-c37b459226bf",
   "metadata": {},
   "source": [
    "## Gradient of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477d1b7-f249-44f8-b8f3-cd027096eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18122430-26c8-45ef-95e0-a56eb56a4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_model(x):\n",
    "    return jnp.sum(sum(s.apply_fn({'params': s.params}, x) for s in state[:]))\n",
    "    \n",
    "grad_func_model = jit(grad(func_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4ea4c-c1d2-447e-846a-77974beb6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit grad_func_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e79a16-2252-4a55-be2c-23ee70feff46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ccc4c-cb5b-4702-b6ec-ff4247d6bfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68f377-0a09-4a83-a234-d2adad235c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecc24e-5ca4-41df-8fa5-7ca33fa832d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x):\n",
    "    return jnp.sin(x).sum()\n",
    "\n",
    "def scale(x):\n",
    "    return 2 * x - 1\n",
    "    \n",
    "def func(x):\n",
    "    return fn(scale(x))\n",
    "    \n",
    "grad_fn = grad(fn)\n",
    "grad_func = grad(func)\n",
    "\n",
    "jnp.all(grad_func(x) == 2 * grad_fn(scale(x)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c1a57-d130-41ba-9be2-84808531fdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8ac20-df29-4858-9525-987aa99a12e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede45d08-95d8-4d77-8290-13b216d52a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4d6d7-c49d-41c1-82e8-5450ddb8a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(nn.Module):\n",
    "    features: int\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        kernel = self.param('kernel',\n",
    "                            self.kernel_init, # Initialization function\n",
    "                            (inputs.shape[-1], self.features))  # shape info.\n",
    "        y = lax.dot_general(inputs, kernel,\n",
    "                            (((inputs.ndim - 1,), (0,)), ((), ())),) # TODO Why not jnp.dot?\n",
    "        bias = self.param('bias', self.bias_init, (self.features,))\n",
    "        y = y + bias\n",
    "        return y\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = SimpleDense(features=3)\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameters:\\n', params)\n",
    "print('output:\\n', y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlpot3)",
   "language": "python",
   "name": "mlpot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
