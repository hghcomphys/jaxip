{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Client and PyTorch\n",
    "\n",
    "Experiments on how to parallelize Pytorch tensors using Dask client. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dask client (multi-process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, fire_and_forget\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "client = Client(memory_limit='4GB', n_workers=2, processes=True, threads_per_worker=2, dashboard_address=':8791')\n",
    "\n",
    "# cluster = LocalCUDACluster(n_workers=1, threads_per_worker=1, dashboard_address=':8791',\n",
    "#                               memory_limit=\"auto\",\n",
    "#                               device_memory_limit=\"auto\", # memory spilling\n",
    "#                               #rmm_pool_size=\"5GB\",\n",
    "#                               #rmm_managed_memory=True,\n",
    "#                               #silence_logs=False,\n",
    "#                               local_directory=\"/tmp/\", \n",
    "#                               #enable_nvlink=True,\n",
    "#                               ) # See https://docs.rapids.ai/api/dask-cuda/nightly/api.html\n",
    "# client = Client(cluster)\n",
    "\n",
    "display(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "N = 1000\n",
    "\n",
    "class A:\n",
    "    def __init__(self):\n",
    "        self.x = torch.rand(N, N)\n",
    "\n",
    "class Kernel:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        \n",
    "    def kernel(self, obj):\n",
    "        self.count += 1\n",
    "        y = torch.rand(N, N)\n",
    "        for _ in range(10):\n",
    "            y = y + torch.rand_like(y)\n",
    "        # return torch.matmul(obj.x, y)  # this cannot be parallelized by Dask client.\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op = Kernel()\n",
    "# for _ in range(500):\n",
    "#     obj = A()\n",
    "#     obj_ = client.scatter(obj)\n",
    "#     future = client.submit(op.kernel, obj_)\n",
    "#     fire_and_forget(future)\n",
    "# # op.kernel(A())\n",
    "# # op.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future: Tensors\n",
    "\n",
    "Observations:\n",
    "- There is an issue of transfering the history of pytorch graph for a tensor when using the multi-process dask client.\n",
    "- Multi-thread client is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x, y):\n",
    "    # r = torch.zeros(x.shape)\n",
    "    # for _ in range(1000):\n",
    "    #     if x.max() > y.max():\n",
    "    #         r = r + torch.sin(x+y)\n",
    "    #     else:\n",
    "    #         r = r + torch.cos(x-y)\n",
    "    # return r\n",
    "    return torch.sin(x+y)\n",
    "\n",
    "def gradient(y, x, grad_outputs=None):\n",
    "    if grad_outputs is None:\n",
    "        grad_outputs = torch.ones_like(y)\n",
    "    grad = torch.autograd.grad(y, [x], grad_outputs = grad_outputs, create_graph=True)[0]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=100\n",
    "\n",
    "x = torch.rand(size, requires_grad=True)\n",
    "y = torch.rand(size, requires_grad=False)\n",
    "tensors = [x, y]\n",
    "\n",
    "futures = []\n",
    "for _ in range(10):\n",
    "    scattered_tensors = client.scatter(tensors, broadcast=True)\n",
    "    future = client.submit(kernel, *scattered_tensors)\n",
    "    futures.append(future)\n",
    "results = client.gather(futures)\n",
    "\n",
    "gradient(results[0], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future: structures\n",
    "\n",
    "Observations:\n",
    "- Dask cannot directly parallize a kernel with generic input class such as Structure.\n",
    "- As a solution, the kernel's inputs have to be translated in form of arrays or tensors. \n",
    "- Also defining a Kernel class which takes care of unnecessary inputs are very useful and an elegant design. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torchip as tp\n",
    "from torchip import logger\n",
    "from torchip.datasets import RunnerStructureDataset, ToStructure\n",
    "from torchip.potentials import NeuralNetworkPotential\n",
    "\n",
    "tp.device.DEVICE = \"cpu\"\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potdir = Path(\"../examples/LJ\")\n",
    "\n",
    "structures = RunnerStructureDataset(Path(potdir, \"input.data\"), persist=True) \n",
    "structure0 = structures[4]\n",
    "\n",
    "nnp = NeuralNetworkPotential(Path(potdir, \"input.nn\"))\n",
    "descriptor = nnp.descriptor[\"Ne\"]\n",
    "scaler = nnp.scaler[\"Ne\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure0.calculate_distance(aid=0, neighbors=1, detach=False, return_diff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "class Box:\n",
    "    def __init__(self, lattice):\n",
    "        self.lattice = lattice\n",
    "    \n",
    "    @staticmethod\n",
    "    def _apply_pbc(dx, lat):\n",
    "        for i in range(3):\n",
    "            l = lat[i, i]\n",
    "            dx[..., i] = torch.where(dx[..., i] >  0.5E0*l, dx[..., i] - l, dx[..., i])\n",
    "            dx[..., i] = torch.where(dx[..., i] < -0.5E0*l, dx[..., i] + l, dx[..., i])\n",
    "        return dx\n",
    "    \n",
    "    def apply_pbc(self, dx):\n",
    "        return Box._apply_pbc(dx, self.lattice)\n",
    "        \n",
    "\n",
    "class Structure:\n",
    "    @staticmethod\n",
    "    def _calculate_distance(\n",
    "            pos: Tensor,\n",
    "            aid: int, \n",
    "            lat: Tensor = None,\n",
    "            detach: bool = False, \n",
    "            neighbors = None, \n",
    "            difference: bool = False\n",
    "        ) -> Tensor: # TODO: also tuple?\n",
    "        \"\"\"\n",
    "        This method calculates an array of distances of all atoms existing in the structure from an input atom. \n",
    "        TODO: input pbc flag, using default pbc from global configuration\n",
    "        TODO: also see torch.cdist\n",
    "        \"\"\"   \n",
    "        x = pos.detach() if detach else pos\n",
    "        x = x[neighbors] if neighbors else x \n",
    "        x = torch.unsqueeze(x, dim=0) if x.ndim == 1 else x  # for when neighbors index is only a number\n",
    "        dx = pos[aid] - x  # FIXME: detach?\n",
    "\n",
    "        # Apply PBC along x,y,and z directions if lattice info is provided \n",
    "        if lat is not None:\n",
    "            dx = Box._apply_pbc(dx, lat) # using broadcasting\n",
    "\n",
    "        # Calculate distance from dx tensor\n",
    "        distance = torch.linalg.vector_norm(dx, dim=1)\n",
    "\n",
    "        return distance if not difference else (distance, dx)\n",
    "    \n",
    "    \n",
    "class Kernel:\n",
    "    def __init__(self, func, dist, pbc):\n",
    "        self.func = func\n",
    "        self.dist = dist\n",
    "        self.pbc = pbc\n",
    "        \n",
    "    def __call__(self, x, at, dtype=None, device=None, emap=None, lat=None):\n",
    "        for i in range(190000):\n",
    "            self.func(x)\n",
    "            self.func(at)\n",
    "        if emap:\n",
    "            emap[int(at[0])]\n",
    "        if self.dist:\n",
    "            dx = self.dist(x, aid=0, neighbors=1)\n",
    "            print(dx)\n",
    "        if lat:\n",
    "            self.pbc(dx, lat)\n",
    "        # time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel(torch.max, dist=Structure._calculate_distance, pbc=Box._apply_pbc)\n",
    "\n",
    "for structure in structures:\n",
    "    tensors = [\n",
    "        structure.position, \n",
    "        structure.atype\n",
    "    ]\n",
    "    params = {\n",
    "        'dtype': torch.double, \n",
    "        'device': 'cpu', \n",
    "        'emap': structure.element_map.atype_to_element,\n",
    "        'lat': structure.box.lattice if structure.box else None,\n",
    "    }\n",
    "    scattered_tensors = client.scatter(tensors, broadcast=True)  \n",
    "    future = client.submit(kernel, *scattered_tensors, **params)\n",
    "    fire_and_forget(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dask import delayed\n",
    "\n",
    "# @torch.jit.script\n",
    "def fun(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x\n",
    "\n",
    "# fn = delayed(fun, pure=False)  # works\n",
    "fn = delayed(fun, pure=True)  # causes error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn(torch.rand(size, requires_grad=True, dtype=dtype)).compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torchip)",
   "language": "python",
   "name": "torhcip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
